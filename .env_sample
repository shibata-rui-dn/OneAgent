# ===========================================
# Dynamic Tool MCP Server + AI Agent 設定（ローカルLLM対応版）
# ===========================================
# このファイルをコピーして .env として保存し、設定を編集してください
# cp .env_sample .env

# ===========================================
# 必須設定
# ===========================================

# OpenAI API Key（OpenAI/Azure OpenAI使用時のみ必須）
# https://platform.openai.com/account/api-keys から取得
# ローカルLLM使用時は不要
OPENAI_API_KEY=your_openai_api_key_here

# ===========================================
# AIプロバイダー設定
# ===========================================

# AIプロバイダー (openai, azureopenai, または localllm)
AI_PROVIDER=openai

# 使用するモデル
# OpenAI: gpt-4o-mini, gpt-4o, gpt-3.5-turbo
# Azure OpenAI: デプロイメント名
# ローカルLLM: AI_MODEL設定は無視され、LOCAL_LLM_MODELが使用されます
AI_MODEL=gpt-4o-mini

# ストリーミング応答の有効/無効
AI_STREAMING=true

# 応答の創造性 (0.0 = 決定論的, 2.0 = 非常に創造的)
AI_TEMPERATURE=0.5

# 最大トークン数 (レスポンスの長さ制限)
AI_MAX_TOKENS=8024

# ===========================================
# サーバー設定
# ===========================================

# サーバーポート番号
PORT=3000

# サーバーホスト (通常は localhost のまま)
HOST=localhost

# ===========================================
# Azure OpenAI 設定（Azure OpenAI使用時のみ）
# ===========================================

# Azure OpenAI を使用する場合は以下のコメントアウトを解除して設定

# Azure OpenAI エンドポイント
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com

# Azure OpenAI API バージョン
# AZURE_OPENAI_API_VERSION=2024-02-15-preview

# ===========================================
# ローカルLLM設定（AI_PROVIDER=localllm使用時のみ）
# ===========================================

# ローカルLLM（VLLM）のベースURL
# VLLMサーバーが localhost:8000 で動作している場合のデフォルト設定
LOCAL_LLM_URL=http://localhost:8000

# ローカルLLMで使用するモデル名
# VLLMサーバーで読み込まれているモデル名を指定
LOCAL_LLM_MODEL=Qwen/Qwen3-4B
